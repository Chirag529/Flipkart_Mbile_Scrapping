{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib3.exceptions import InsecureRequestWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable insecure request warnings\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "# Lists to store scraped data\n",
    "Product_Name = []\n",
    "Product_Price = []\n",
    "Product_Rating = []\n",
    "Product_Desc = []\n",
    "\n",
    "# Scrape data from multiple pages (2 to 4)\n",
    "for i in range(2, 5):                             # Change the number to fetch more pages\n",
    "    url = \"https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\" + str(i)\n",
    "\n",
    "    # Send request to the URL and get the response\n",
    "    response = requests.get(url, verify=False)\n",
    "\n",
    "    # Check if the response is successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        soup = bs(response.text, \"lxml\")\n",
    "\n",
    "        # Find the link to the next page (NEXT Button link)\n",
    "        link = soup.find(\"a\", attrs={\"class\": \"_1fQZEK\"}).get(\"href\")\n",
    "        product_list = \"https://flipkart.com\" + link\n",
    "\n",
    "        # Find the div containing mobile data on the page\n",
    "        box = soup.find(\"div\", attrs={\"class\": \"_1YokD2 _3Mn1Gg\"})\n",
    "\n",
    "        # Fetch Name, Price, Rating, and Description of each mobile\n",
    "        devices = box.find_all(\"div\", attrs={'class': \"_4rR01T\"})\n",
    "        for i in devices:\n",
    "            # Scrape product name from website\n",
    "            name = i.text\n",
    "            Product_Name.append(name)\n",
    "\n",
    "        prices = box.find_all(\"div\", attrs={'class': \"_30jeq3 _1_WHN1\"})\n",
    "        for i in prices:\n",
    "            # Scrape product price from website\n",
    "            price = i.text.strip()\n",
    "            Product_Price.append(price)\n",
    "\n",
    "        rating = box.find_all(\"div\", attrs={'class': \"_3LWZlK\"})\n",
    "        for i in rating:\n",
    "            # Scrape product rating from website\n",
    "            rating = i.text.strip()\n",
    "            Product_Rating.append(rating)\n",
    "\n",
    "        details = box.find_all(\"ul\", attrs={'class': \"_1xgFaf\"})\n",
    "        for i in details:\n",
    "            # Scrape product details from website\n",
    "            details = i.text.strip()\n",
    "            Product_Desc.append(details)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage. Status Code: {response.status_code}\")\n",
    "\n",
    "# Create a DataFrame and save data to CSV\n",
    "df = pd.DataFrame({\"Product Name\": Product_Name, \"Product Price\": Product_Price, \"Product Rating\": Product_Rating, \"Product Details\": Product_Desc})\n",
    "df.to_csv('Flipkart_Mobiles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
